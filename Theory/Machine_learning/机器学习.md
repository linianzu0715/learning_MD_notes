### 1. 优化器算法

首先来看一下梯度下降最常见的三种变形 BGD，SGD，MBGD，
这三种形式的区别就是取决于我们用多少数据来计算目标函数的梯度，
这样的话自然就涉及到一个 trade－off，即参数更新的准确率和运行时间。

#### 1. Batch gradient descent

**梯度更新规则:**
BGD 采用整个训练集的数据来计算 cost function 对参数的梯度：

![img](https://img-blog.csdn.net/20180622123704227?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1ODYwMzUy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**缺点:**
由于这种方法是在一次更新中，就对整个数据集计算梯度，所以计算起来非常慢，遇到很大量的数据集也会非常棘手，而且不能投入新数据实时更新模型

```python
for i in range(nb_epochs):
  params_grad = evaluate_gradient(loss_function, data, params)
  params = params - learning_rate * params_grad
```

我们会事先定义一个迭代次数 epoch，首先计算梯度向量 params_grad，然后沿着梯度的方向更新参数 params，learning rate 决定了我们每一步迈多大。

Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值。



#### 2. Stochastic gradient descent

**梯度更新规则:**和 BGD 的一次用所有数据计算梯度相比，SGD 每次更新时对每个样本进行梯度更新，
对于很大的数据集来说，可能会有相似的样本，这样 BGD 在计算梯度时会出现冗余，
而 SGD 一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本。

![img](https://upload-images.jianshu.io/upload_images/1667471-a2f485d5c8f2de47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/326)

```python
for i in range(nb_epochs):
  np.random.shuffle(data)
  for example in data:
    params_grad = evaluate_gradient(loss_function, example, params)
    params = params - learning_rate * params_grad
```

看代码，可以看到区别，就是整体数据集是个循环，其中对每个样本进行一次参数更新。

**缺点:**
但是 SGD 因为更新比较频繁，会造成 cost function 有严重的震荡。

![img](https://upload-images.jianshu.io/upload_images/1667471-a024fa954fd7e10b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/363)

BGD 可以收敛到局部极小值，当然 SGD 的震荡可能会跳到更好的局部极小值处。

当我们稍微减小 learning rate，SGD 和 BGD 的收敛性是一样的。



#### 3. Mini-batch gradient descent

**梯度更新规则:**
MBGD 每一次利用一小批样本，即 n 个样本进行计算，
这样它可以降低参数更新时的方差，收敛更稳定，
另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算。

![img](https://upload-images.jianshu.io/upload_images/1667471-6a7a813d59964570.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/355)

和 SGD 的区别是每一次循环不是作用于每个样本，而是具有 n 个样本的批次。

```python
for i in range(nb_epochs):
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params)
    params = params - learning_rate * params_grad
```

**超参数设定值:**
n 一般取值在 50～256

缺点:
不过 Mini-batch gradient descent 不能保证很好的收敛性，

1. learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。
   有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点。

2. 此外，这种方法是对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。

3. 另外，对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error 是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里。

鞍点就是：一个光滑函数的鞍点邻域的曲线，曲面，或超曲面，都位于这点的切线的不同边。
例如这个二维图形，像个马鞍：在x-轴方向往上曲，在y-轴方向往下曲，鞍点就是（0，0）



为了应对上面的三点挑战就有了下面这些算法。

［应对挑战 1］

#### 4. Momentum

SGD 在 ravines 的情况下容易被困住， ravines 就是曲面的一个方向比另一个方向更陡，这时 SGD 会发生震荡而迟迟不能接近极小值：


梯度更新规则:
Momentum 通过加入 γv_t−1 ，可以加速 SGD， 并且抑制震荡

当我们将一个小球从山上滚下来时，没有阻力的话，它的动量会越来越大，但是如果遇到了阻力，速度就会变小。
加入的这一项，可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。

**超参数设定值:**
一般 γ 取值 0.9 左右。

**缺点:**
这种情况相当于小球从山上滚下来时是在盲目地沿着坡滚，如果它能具备一些先知，例如快要上坡时，就知道需要减速了的话，适应性会更好。



#### 5. Nesterov accelerated gradient

**梯度更新规则:**

但是，一个球从山坡上滚下，仅仅是盲目地沿着坡道的方向，这不是令人满意的。我们想要有一个更“聪明”的球，这个球可以知道它将要去哪里，也就是说，它知道在遇到一个上坡之前，先降低自己的速度。

Nesterov accelerated gradient (NAG)是一种给我们的momentum方法提供先知能力的方式。我们知道我们会使用momentum项$\gamma v_{t-1}$来移动我们的参数$\theta$,我们用$\theta - \gamma v_{t-1}$来近似下一个$\theta$的位置。这是一种粗略的估计。现在哦我们可以有效的知道$\theta$的位置，通过计算梯度，并非现在参数的梯度，而是参数在未来位置的梯度：

$$vt=γvt−1+η∇θJ(θ−γvt−1)$$

$$θ=θ−vt$$



**超参数设定值:**
γ 仍然取值 0.9 左右。



**效果比较:**

蓝色是 Momentum 的过程，会先计算当前的梯度，然后在更新后的累积梯度后会有一个大的跳跃。
而 NAG 会先在前一步的累积梯度上(brown vector)有一个大的跳跃，然后衡量一下梯度做一下修正(red vector)，这种预期的更新可以避免我们走的太快。NAG 可以使 RNN 在很多任务上有更好的表现。

![nesterov_update_vector](C:\Users\59584\Desktop\nesterov_update_vector.png)

目前为止，我们可以做到，在更新梯度时顺应 loss function 的梯度来调整速度，并且对 SGD 进行加速。

我们还希望可以根据参数的重要性而对不同的参数进行不同程度的更新。



［应对挑战 2］

#### 6. Adagrad

这个算法就可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性，例如识别 Youtube 视频里面的猫，训练 GloVe word embeddings，因为它们都是需要在低频的特征上有更大的更新。

在学习过程中学习率不是不变的，我们先设置一个全局学习率，在更新梯度时候，梯度要初一历史梯度的模值的平方和，因此每个参数都有不同的学习率。

Adagrad [[9\]](https://ruder.io/optimizing-gradient-descent/index.html#fn9) 是一个基于梯度的优化算法，它对参数选择合适的学习率，采取更加智能的更新。对于频繁出现的特征的参数，我们选择较低的更新率，对于不频繁出现的特征的参数，我们给与较高的更新率。因为这样能更好的适应稀疏数据。Dean et al. [[10\]](https://ruder.io/optimizing-gradient-descent/index.html#fn10) 很好地提高了 SGD 的鲁棒性 例如识别 Youtube 视频里面的猫，训练 GloVe word embeddings，因为它们都是需要在低频的特征上有更大的更新。

之前，我们对参数$\theta$的更新都是使用的统一的学习率。随着Adagrad使用了不同的学习率对应不同的$\theta i$在步骤t中，我们首先显示Adagrad对应于每个参数的更新，我们成为向量化（vectorize).为了简化，我们用gt来表示在步骤t时候的梯度。$g_{t, i}$是目标函数的偏微分对应于参数$\theta i$在步骤t时候。

$$gt,i=∇θJ(θt,i)$$

SGD 更新对应于参数$\theta i$在每个步骤t时候为：

$$\theta_{t+1, i} = \theta_{t, i} - \eta \cdot g_{t, i}$$

根据这种更新规则，Adagrad修改通用的学习率$η$，在步骤t，对应于每个参数$θi$ ，基于用来计算$θi$ 的过去所有的梯度：

$$\theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}$$

$G_{t} \in \mathbb{R}^{d \times d}$ here is a diagonal matrix where each diagonal element i,i is the sum of the squares of the gradients w.r.t. θi up to time step $t$ [[12\]](https://ruder.io/optimizing-gradient-descent/index.html#fn12), while ϵ is a smoothing term that avoids division by zero (usually on the order of 1e−8). Interestingly, without the square root operation, the algorithm performs much worse.

As $G_{t}$ contains the sum of the squares of the past gradients w.r.t. to all parameters θ along its diagonal, we can now vectorize our implementation by performing a matrix-vector product ⊙ between Gt and gt 

$$\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{G_{t} + \epsilon}} \odot g_{t}$$

Adagrad的一个主要的好处是它不需要手动的调整学习率的值。大多数情况下我们只需要使用默认的值0.01就行

Adagrad的主要的缺点是在分母部分的梯度的平方和。因为每个平方和的项都是正值，因此会在训练的过程中不断累积。这就导致学习率快速下降，最终学习率很低，因此参数不会再有变化。



#### 7. Adadelta

这个算法是对 Adagrad 的改进，和 Adagrad 相比，就是分母的 G 换成了过去的梯度平方的衰减平均值，

其中 E 的计算公式如下，t 时刻的依赖于前一时刻的平均和当前的梯度：

$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t$

这个分母相当于梯度的均方根 root mean squared (RMS) ，所以可以用 RMS 简写：

$\Delta \theta_t = - \dfrac{\eta}{RMS[g]_{t}} g_t$



#### 8. RMSprop



#### 9. Adam

代替传统的梯度下降的一种参数的更新方式，使用一阶矩估计和二阶矩估计，为不同的参数提供不同的更新率。

#### 效果比较?



link：

https://ruder.io/optimizing-gradient-descent/index.html#nesterovacceleratedgradient









